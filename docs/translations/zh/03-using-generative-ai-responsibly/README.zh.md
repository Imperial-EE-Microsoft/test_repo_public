# 负责任地使用生成型人工智能

[![负责任地使用生成型人工智能](./images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]()

> **视频即将上线**

人工智能，特别是生成型人工智能，令人着迷，但你需要考虑如何负责任地使用它。你需要考虑如何确保输出是公正的、非有害的等。本章旨在为你提供上下文信息，让你了解应该考虑哪些因素以及如何采取积极步骤来改善你的人工智能使用。

## 介绍

本课程将涵盖：

- 为什么在构建生成型人工智能应用程序时，你应该优先考虑负责任的人工智能。
- 负责任人工智能的核心原则及其与生成型人工智能的关系。
- 如何通过战略和工具将这些负责任人工智能原则付诸实践。

## 学习目标

完成本课程后，你将了解：

- 在构建生成型人工智能应用程序时，负责任人工智能的重要性。
- 在构建生成型人工智能应用程序时何时考虑和应用负责任人工智能的核心原则。
- 实现负责任人工智能概念的可用工具和策略。

## 负责任人工智能原则

生成型人工智能的兴奋感从未如此之高。这种兴奋带来了许多新的开发人员、关注度和资金。虽然这对于任何想要使用生成型人工智能构建产品和公司的人来说都非常积极，但我们也需要负责任地前进。

在本课程中，我们专注于构建我们的初创公司和我们的人工智能教育产品。我们将使用负责任人工智能的原则：公正、包容、可靠/安全、安全和隐私、透明和问责。通过这些原则，我们将探讨它们与我们在产品中使用生成型人工智能的关系。

## 为什么你应该优先考虑负责任人工智能

在构建产品时，以人为本，考虑用户的最佳利益会带来最好的结果。

生成型人工智能的独特之处在于它可以为用户创建有用的答案、信息、指导和内容。这可以在不需要太多手动步骤的情况下完成，从而可以获得非常出色的结果。但是，如果没有适当的规划和策略，它也可能不幸地导致对用户、产品和整个社会造成一些有害的结果。

让我们看一些（但不是全部）潜在的有害结果：

### 幻觉

“幻觉”是一个术语，用来描述LLM生成的内容是完全没有意义的或者是我们根据其他信息来源知道是事实错误的。

例如，我们为我们的初创公司构建了一个功能，允许学生向模型提问历史问题。学生提出问题“泰坦尼克号的唯一幸存者是谁？”

模型生成了下面的响应：

![Prompt saying "Who was the sole survivor of the Titanic"](../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(来源：[Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

这是一个非常自信和全面的答案。不幸的是，它是不正确的。即使进行最少量的研究，人们也会发现泰坦尼克号灾难的幸存者不止一个。对于刚开始研究这个话题的学生来说，这个答案可能足够有说服力，以至于不会被质疑并被视为事实。这可能导致AI系统不可靠，并对我们初创公司的声誉产生负面影响。

对于任何给定的LLM的每个迭代，我们都看到了在最小化幻觉方面的性能改进。即使有了这种改进，我们作为应用程序构建者和用户仍然需要注意这些限制。

### 有害内容

我们在早期的部分中介绍了LLM生成不正确或无意义的响应。我们需要注意的另一个风险是当模型响应有害内容时。

有害内容可以定义为：

- 提供指导或鼓励自我伤害或对某些群体造成伤害。
- 充满仇恨或贬低的内容。
- 指导计划任何类型的攻击或暴力行为。
- 提供有关如何查找非法内容或实施非法行为的指导。
- 显示性爱内容。

对于我们的初创公司，我们希望确保我们有正确的工具和策略，以防止学生看到这种类型的内容。

### 公正性的缺乏

公正性被定义为“确保AI系统没有偏见和歧视，并且他们公平平等地对待每个人。”在生成型人工智能的世界中，我们希望确保边缘化群体的排斥性世界观不会被模型的输出所强化。

这些类型的输出不仅破坏了为我们的用户构建积极产品体验的机会，而且还会对社会造成进一步的伤害。作为应用程序构建者，在使用生成型人工智能构建解决方案时，我们应始终考虑广泛和多样化的用户群体。

## 如何负责任地使用生成型人工智能

现在，我们已经确定了负责任的生成型人工智能的重要性，让我们看一下我们可以采取的4个步骤来负责任地构建我们的AI解决方案：

![减轻周期](./images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 测量潜在危害

在软件测试中，我们测试用户在应用程序上的预期操作。同样，测试用户最可能使用的各种提示集是衡量潜在危害的好方法。

由于我们的初创公司正在构建一款教育产品，因此准备一个教育相关提示列表将是一个好方法。这可以涵盖某个主题、历史事实以及有关学生生活的提示。

### 减轻潜在危害

现在是时候找到方法来防止或限制模型及其响应引起的潜在危害了。我们可以从4个不同的层面来看这个问题：

![缓解层](./images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **模型**。为正确的用例选择正确的模型。像GPT-4这样更大、更复杂的模型在应用于更小、更具体的用例时会导致更多的有害内容风险。使用你的训练数据进行微调也会减少有害内容的风险。

- **安全系统**。安全系统是在服务于模型的平台上设置的一组工具和配置，可以帮助减轻危害。Azure OpenAI服务上的内容过滤系统就是一个例子。系统还应检测越狱攻击和不需要的活动，如来自机器人的请求。

- **元提示**。元提示和接地是我们可以基于某些行为和信息来指导或限制模型的方式。这可以使用系统输入来定义模型的某些限制。此外，提供更符合系统范围或领域的输出。

 这也可以使用检索增强生成（RAG）之类的技术，使模型仅从一组可信源中提取信息。本课程后面还有一节课，介绍[构建搜索应用程序](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)。

- **用户体验**。最后一层是用户通过我们应用程序的界面直接与模型交互的地方。这样，我们可以设计UI/UX，限制用户向模型发送的输入类型以及向用户显示的文本或图像。在部署AI应用程序时，我们还必须透明地说明我们的生成型人工智能应用程序可以做什么和不能做什么。

我们有一整节课程专门介绍[为AI应用程序设计UX](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)。

- **评估模型**。与LLM一起工作可能具有挑战性，因为我们并不总是控制模型所训练的数据。尽管如此，我们仍然应该评估模型的性能和输出。仍然重要的是测量模型的准确性、相似性、接地性和输出的相关性。这有助于向利益相关者和用户提供透明度和信任。

### 运营负责任的生成型人工智能解决方案

建立围绕你的AI应用程序的操作实践是最后一个阶段。这包括与我们初创公司的其他部门合作，如法律和安全，以确保我们符合所有监管政策。在启动之前，我们还要建立交付、处理事件和回滚计划，以防止对我们的用户造成任何伤害。

## 工具

虽然开发负责任的AI解决方案的工作可能看起来很多，但这些工作是值得付出努力的。随着生成型人工智能领域的发展，更多的工具可以帮助开发人员有效地将负责任性集成到他们的工作流程中。例如，[Azure AI内容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)可以通过API请求检测有害内容和图像。

## 知识检查

你需要关心哪些事情来确保负责任的AI使用？

1. 答案是否正确。
2. 有害使用，即不将AI用于犯罪目的。
3. 确保AI不受偏见和歧视的影响。

A: 2和3是正确的。负责任的AI帮助你考虑如何减轻有害影响和偏见等。

## 🚀 挑战

阅读有关[Azure AI内容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)的内容，并看看你可以为自己的使用采用什么。

## 做得好，继续学习

完成本课程后，请查看我们的[生成型人工智能学习集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，继续提高你的生成型人工智能知识！

现在前往第4课，我们将学习[提示工程基础知识](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)！


免责声明：该翻译是由AI模型翻译的，可能不完美。请查看输出并进行必要的更正。