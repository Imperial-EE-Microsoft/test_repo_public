# 负责任地使用生成式人工智能

[![负责任地使用生成式人工智能](./images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **即将推出视频**

人工智能，尤其是生成式人工智能，很容易引起人们的兴趣，但你需要考虑如何负责任地使用它。你需要考虑如何确保输出公正、无害等问题。本章旨在为您提供上下文信息，告诉您需要考虑什么，以及如何采取积极措施来改善您的人工智能使用。

## 简介

本课程将涵盖：

- 为什么在构建生成式人工智能应用程序时应优先考虑负责任的人工智能。
- 负责任人工智能的核心原则及其与生成式人工智能的关系。
- 如何通过策略和工具将这些负责任人工智能原则付诸实践。

## 学习目标

完成本课程后，您将了解：

- 构建生成式人工智能应用程序时负责任人工智能的重要性。
- 在构建生成式人工智能应用程序时何时思考和应用负责任人工智能的核心原则。
- 实现负责任人工智能概念的可用工具和策略。

## 负责任人工智能原则

生成式人工智能的激动人心程度从未如此之高。这种兴奋带来了许多新的开发人员、关注和资金。虽然这对于任何想要使用生成式人工智能构建产品和公司的人来说都非常积极，但我们也必须以负责任的方式前进。

在本课程中，我们将专注于构建我们的创业公司和我们的人工智能教育产品。我们将使用负责任人工智能的原则：公正、包容性、可靠性/安全性、安全和隐私、透明度和问责制。借助这些原则，我们将探讨它们与我们在产品中使用生成式人工智能的关系。

## 为什么应优先考虑负责任人工智能

在构建产品时，以人为本的方法是保持用户最佳利益的最佳结果。

生成式人工智能的独特之处在于它可以为用户创建有用的答案、信息、指导和内容。这可以在不需要进行许多手动步骤的情况下完成，从而可以获得非常出色的结果。如果没有适当的规划和策略，它也可能不幸地导致对您的用户、产品和整个社会产生一些有害的结果。

让我们看看一些（但不是全部）潜在的有害结果：

### 幻觉

“幻觉”是一个用来描述LLM产生的内容完全没有意义或基于其他信息来源，我们知道它是事实错误的术语。

例如，我们为我们的创业公司构建了一个功能，允许学生向模型提问历史问题。一个学生提出了问题“泰坦尼克号的唯一幸存者是谁？”

模型生成了如下回答：

![Prompt saying "Who was the sole survivor of the Titanic"](../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(来源: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

这是一个非常自信和详细的答案。不幸的是，它是不正确的。即使进行最少的研究，人们也会发现泰坦尼克号灾难有不止一个幸存者。对于刚开始研究这个话题的学生来说，这个答案可能足够有说服力，不会被质疑并被视为事实。这样做的后果可能会导致AI系统不可靠，并对我们的创业公司声誉产生负面影响。

在任何给定LLM的每次迭代中，我们已经看到了最小化幻觉的性能改进。即使有了这个改进，我们作为应用程序构建者和用户仍然需要注意这些限制。

### 有害内容

我们在早期部分介绍了当LLM产生不正确或无意义的响应时。我们需要注意的另一个风险是模型响应有害内容。

有害内容可以定义为：

- 提供指令或鼓励自残或对某些团体造成伤害。
- 充满仇恨或贬低的内容。
- 引导规划任何类型的攻击或暴力行为。
- 提供有关如何查找非法内容或犯罪行为的指示。
- 显示性暴力内容。

对于我们的创业公司，我们希望确保我们拥有适当的工具和策略，以防止学生看到此类内容。

### 缺乏公正性

公正性被定义为“确保AI系统没有偏见和歧视，而且对每个人都公平和平等地对待”。在生成式人工智能的世界中，我们希望确保不会通过模型的输出强化边缘化群体的排他性世界观。

这些类型的输出不仅对于为我们的用户构建积极的产品体验具有破坏性，而且还会导致进一步的社会伤害。作为应用程序构建者，在使用生成式人工智能构建解决方案时，我们应始终牢记广泛和多样化的用户群体。

## 如何负责任地使用生成式人工智能

既然我们已经确定了负责任的生成式人工智能的重要性，让我们看看可以采取哪些步骤来负责任地构建我们的AI解决方案：

![减轻周期](./images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 测量潜在的危害

在软件测试中，我们测试用户在应用程序上预期的操作。同样，测试用户最有可能使用的各种提示是衡量潜在危害的好方法。

由于我们的创业公司正在构建一种教育产品，因此准备一份教育相关提示列表是个好主意。这可以涵盖某个主题、历史事实和有关学生生活的提示。

### 减轻潜在的危害

现在是时候找到可以防止或限制模型及其响应引起潜在危害的方法了。我们可以从以下4个不同的层面考虑这个问题：

![减轻层](./images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **模型**。为正确的用例选择正确的模型。像GPT-4这样更大、更复杂的模型在应用于更小、更具体的用例时可能会导致更多有害内容的风险。使用您的训练数据进行微调也可以降低有害内容的风险。

- **安全系统**。安全系统是平台上提供模型的一组工具和配置，可帮助减轻危害。Azure OpenAI服务上的内容过滤系统就是一个例子。系统还应检测越狱攻击和不需要的活动，例如来自机器人的请求。

- **元提示**。元提示和接地是我们可以基于某些行为和信息指导或限制模型的方式。这可以使用系统输入来定义模型的某些限制。此外，提供与系统范围或领域更相关的输出。

 还可以使用Retrieval Augmented Generation（RAG）等技术，使模型仅从一组可信源中提取信息。本课程后面还有一节关于[构建搜索应用程序](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)的课程。

- **用户体验**。最后一层是用户通过我们应用程序的界面以某种方式直接与模型进行交互。通过这种方式，我们可以设计UI/UX以限制用户发送到模型的输入类型以及向用户显示的文本或图像。在部署AI应用程序时，我们还必须透明地说明我们的生成式人工智能应用程序可以做什么和不能做什么。

 我们有一整节课程[为AI应用程序设计UX](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **评估模型**。与LLM一起工作可能具有挑战性，因为我们并不总是控制模型训练的数据。不管怎样，我们应始终评估模型的性能和输出。仍然重要的是测量模型的准确性、相似性、接地性和输出的相关性。这有助于为利益相关者和用户提供透明度和信任。

### 操作负责任的生成式人工智能解决方案

构建围绕您的AI应用程序的操作实践是最后一阶段。这包括与我们创业公司的其他部门（如法律和安全）合作，以确保我们符合所有监管政策。在启动之前，我们还要制定关于交付、处理事件和回滚的计划，以防止对我们的用户产生任何伤害。

## 工具

虽然开发负责任的AI解决方案的工作可能看起来很多，但这是值得付出努力的工作。随着生成式人工智能领域的发展，更多的工具将成熟，以帮助开发人员有效地将责任整合到他们的工作流程中。例如，[Azure AI内容安全性](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)可以通过API请求帮助检测有害内容和图像。

## 知识检查

您需要注意哪些事项以确保负责任的AI使用？

1. 答案是否正确。
1. 有害使用，即AI不用于犯罪目的。
1. 确保AI没有偏见和歧视。

A：2和3是正确的。负责任的AI可以帮助您考虑如何减轻有害影响和偏见等问题。

## 🚀 挑战

阅读[Azure AI内容安全性](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)，看看您可以为自己的使用采用什么。

## 做得好，继续学习

完成本课程后，请查看我们的[生成式人工智能学习集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，继续提升您的生成式人工智能知识！

前往第4课，我们将学习[提示工程基础知识](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)！


免责声明：此翻译是由AI模型翻译的原始版本，可能不完美。请查看输出并进行任何必要的更正。