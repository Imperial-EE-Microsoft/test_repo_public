# 负责任地使用生成式人工智能

[![负责任地使用生成式人工智能](./images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **视频即将上线**

人工智能，特别是生成式人工智能，很容易让人着迷，但是您需要考虑如何负责任地使用它。您需要考虑如何确保输出是公平、无害的等等。本章旨在为您提供上下文信息、考虑事项以及如何采取积极措施来改善您的人工智能使用。

## 介绍

本课程将涵盖以下内容：

- 为什么在构建生成式人工智能应用程序时应优先考虑负责任的人工智能。
- 负责任的人工智能的核心原则以及它们与生成式人工智能的关系。
- 如何通过策略和工具将这些负责任的人工智能原则付诸实践。

## 学习目标

完成本课程后，您将了解：

- 在构建生成式人工智能应用程序时负责任的人工智能的重要性。
- 在构建生成式人工智能应用程序时何时考虑并应用负责任的人工智能的核心原则。
- 什么工具和策略可用于将负责任的人工智能概念付诸实践。

## 负责任的人工智能原则

生成式人工智能的兴奋程度从未如此之高。这种兴奋带来了许多新的开发人员、关注和资金。虽然这对于任何想要使用生成式人工智能构建产品和公司的人来说都非常积极，但我们也需要负责任地进行操作。

在整个课程中，我们将专注于构建我们的创业公司和我们的人工智能教育产品。我们将使用负责任的人工智能原则：公平、包容性、可靠性/安全性、安全和隐私、透明度和问责制。有了这些原则，我们将探讨它们与我们在产品中使用生成式人工智能的关系。

## 为什么应该优先考虑负责任的人工智能

在构建产品时，采用以人为本的方法并考虑用户的最佳利益会带来最好的结果。

生成式人工智能的独特之处在于它能够为用户创建有用的答案、信息、指导和内容。这可以在不需要许多手动步骤的情况下完成，从而可以得到非常令人印象深刻的结果。如果没有适当的规划和策略，它也可能不幸地导致对用户、产品和整个社会产生一些有害的结果。

让我们来看看一些（但不是全部）可能产生的有害结果：

### 幻觉

“幻觉”是一个术语，用于描述LLM产生的内容要么完全没有意义，要么基于其他信息源，我们知道是事实上错误的。

例如，我们为我们的创业公司构建了一个功能，允许学生向模型提问历史问题。一个学生提出问题“谁是泰坦尼克号的唯一幸存者？”

模型会产生像下面这样的回答：

![Prompt saying "Who was the sole survivor of the Titanic"](../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(来源：[Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

这是一个非常自信和全面的答案。不幸的是，这是不正确的。即使进行了最少的研究，我们也会发现泰坦尼克号灾难的幸存者不止一个。对于刚开始研究这个话题的学生来说，这个答案可能足够有说服力，不会被质疑并被视为事实。这可能导致AI系统不可靠，并对我们创业公司的声誉产生负面影响。

在任何给定的LLM的每次迭代中，我们都看到了在最小化幻觉方面的性能改进。即使有了这种改进，我们作为应用程序构建者和用户仍然需要注意这些限制。

### 有害内容

在早期的部分中，我们已经介绍了LLM产生不正确或毫无意义的回复的情况。我们需要注意的另一个风险是模型产生有害内容的情况。

有害内容可以定义为：

- 提供指导或鼓励自我伤害或对某些群体造成伤害。
- 充满仇恨或贬低的内容。
- 指导计划任何类型的攻击或暴力行为。
- 提供有关如何找到非法内容或犯罪行为的指导。
- 显示性内容。

对于我们的创业公司，我们希望确保我们拥有正确的工具和策略，以防止学生看到这种类型的内容。

### 公平性的缺乏

公平性被定义为“确保AI系统不带偏见和歧视，以及公平地平等地对待每个人。”在生成式人工智能的世界中，我们希望确保边缘化群体的排除性世界观不会被模型的输出所强化。

这些类型的输出不仅破坏了为用户构建积极的产品体验，而且还会导致进一步的社会伤害。作为应用程序构建者，在构建具有生成式人工智能的解决方案时，我们应始终牢记广泛和多样化的用户群体。

## 如何负责任地使用生成式人工智能

既然我们已经确定了负责任的生成式人工智能的重要性，让我们来看看我们可以采取的四个步骤来负责任地构建我们的AI解决方案：

![减轻周期](./images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 测量潜在危害

在软件测试中，我们测试用户在应用程序上的预期操作。同样，测试用户最有可能使用的各种提示的多样性是衡量潜在危害的好方法。

由于我们的创业公司正在构建一种教育产品，因此准备一份与教育相关的提示列表将是一个好方法。这可以涵盖某个主题、历史事实以及有关学生生活的提示。

### 减轻潜在危害

现在是时候找到可以防止或限制模型及其响应造成潜在伤害的方法了。我们可以从4个不同的层面来考虑这个问题：

![减轻层](./images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **模型**。选择适合特定用例的正确模型。像GPT-4这样的更大更复杂的模型在应用于更小更具体的用例时可能会导致更多有害内容的风险。使用您的训练数据进行微调也可以降低有害内容的风险。

- **安全系统**。安全系统是平台上的一组工具和配置，用于服务于模型并帮助减轻伤害。Azure OpenAI服务上的内容过滤系统就是一个例子。系统还应检测越狱攻击和不需要的活动，例如来自机器人的请求。

- **元提示**。元提示和接地是我们可以根据某些行为和信息来指导或限制模型的方法。这可以是使用系统输入来定义模型的某些限制。此外，提供更与系统范围或领域相关的输出。

 还可以使用检索增强生成（RAG）等技术，使模型仅从一组可信来源中提取信息。本课程后面有一课关于[构建搜索应用程序](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **用户体验**。最后一层是用户通过我们应用程序的界面以某种方式直接与模型交互。通过这种方式，我们可以设计UI/UX以限制用户发送给模型的输入类型以及向用户显示的文本或图像。在部署AI应用程序时，我们还必须透明地说明我们的生成式人工智能应用程序可以做什么和不能做什么。

我们有一整课关于[为AI应用程序设计UX](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **评估模型**。与LLMs合作可能具有挑战性，因为我们并不总是控制模型的训练数据。无论如何，我们都应该始终评估模型的性能和输出。测量模型的准确性、相似性、接地性和输出的相关性仍然很重要。这有助于为利益相关者和用户提供透明度和信任。

### 运营负责任的生成式人工智能解决方案

建立围绕您的AI应用程序的运营实践是最后一个阶段。这包括与我们的创业公司的其他部分合作，例如法律和安全，以确保我们符合所有监管政策。在启动之前，我们还要制定交付、处理事件和回滚计划，以防止对我们的用户造成任何伤害。

## 工具

尽管开发负责任的AI解决方案的工作可能看起来很多，但它是值得付出努力的。随着生成式人工智能领域的发展，更多的工具将帮助开发人员有效地将责任集成到他们的工作流程中。例如，[Azure AI内容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)可以通过API请求帮助检测有害内容和图像。

## 知识检测

为确保负责任的AI使用，您需要关心哪些事项？

1. 答案是否正确。
1. 有害使用，即AI不用于犯罪目的。
1. 确保AI没有偏见和歧视。

答案：2和3是正确的。负责任的AI可以帮助您考虑如何减轻有害效应和偏见等。

## 🚀 挑战

阅读有关[Azure AI内容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)的信息，并查看您可以为自己的使用采用什么。

## 做得好，继续学习

完成本课程后，请查看我们的[生成式人工智能学习集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，以继续提高您的生成式人工智能知识！

请前往第4课，我们将查看[提示工程基础](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)！


免责声明：本翻译是由AI模型翻译的原始文本，可能不完美。请检查输出并进行必要的更正。