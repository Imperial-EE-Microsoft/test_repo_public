possible the response will be. 

### Retrieval Augmented Generation, RAG

Retrieval Augmented Generation (RAG) is a technique that combines the best of two worlds: the accuracy of information retrieval with the flexibility of generative models. RAG models first retrieve a set of relevant documents based on the user's query, and then generate a response based on the retrieved information. This technique ensures that the generated response is grounded in real-world information and can be useful in cases where the LLM does not have access to all the information needed to generate a complete and accurate response.

### Fine-tuned Model

Fine-tuning a pre-trained LLM on a specific dataset is a common approach to improve its performance on a specific task. Fine-tuning involves training the LLM on a smaller dataset of domain-specific examples, which allows the model to adapt to the specific nuances of the domain and improve its accuracy on that task. Fine-tuning can be expensive, but it can lead to significant improvements in the LLM's performance.

## Conclusion

In this lesson, we explored the different types of LLMs, how to select the right model for your use case, and how to test and iterate with different models to understand performance. We also discussed different approaches to improving LLM results, such as prompt engineering with context, retrieval augmented generation, and fine-tuning. With this knowledge, our startup team can now make informed decisions about which LLM to use for their use case and how to improve its performance.
Pour répondre aux attentes de l'utilisateur, la réponse sera basée sur l'apprentissage "one-shot" s'il n'y a qu'un seul exemple dans la requête, et sur l'apprentissage "few-shot" s'il y a plusieurs exemples. L'ingénierie de requêtes avec contexte est l'approche la plus rentable pour démarrer. Les LLM augmentés par récupération (RAG) ont pour limitation qu'ils ne peuvent utiliser que les données utilisées pendant leur formation pour générer une réponse. Cela signifie qu'ils ne connaissent rien des faits qui se sont produits après leur processus de formation, et qu'ils ne peuvent pas accéder aux informations non publiques (comme les données d'entreprise). Cela peut être surmonté grâce à RAG, une technique qui augmente la requête avec des données externes sous forme de fragments de documents, en tenant compte des limites de longueur de la requête. Cette technique est très utile lorsque l'entreprise n'a pas suffisamment de données, de temps ou de ressources pour affiner un LLM, mais souhaite néanmoins améliorer les performances sur une charge de travail spécifique et réduire les risques de falsifications, c'est-à-dire la mystification de la réalité ou le contenu nuisible. Le fine-tuning est un processus qui exploite le transfert d'apprentissage pour "adapter" le modèle à une tâche en aval ou pour résoudre un problème spécifique. Contrairement à l'apprentissage few-shot et RAG, cela résulte en la génération d'un nouveau modèle, avec des poids et des biais mis à jour. La formation d'un LLM à partir de zéro est sans aucun doute l'approche la plus difficile et la plus complexe à adopter, nécessitant des quantités massives de données, des ressources qualifiées et une puissance de calcul appropriée. Cette option ne doit être envisagée que dans un scénario où l'entreprise a un cas d'utilisation spécifique au domaine et une grande quantité de données centrées sur le domaine. Pour améliorer les résultats de complétion des LLM, l'approche recommandée est le fine-tuning si vous avez le temps, les ressources et des données de haute qualité, mais si vous manquez de temps, il est utile de considérer RAG en premier.


Avertissement : La traduction a été effectuée à partir d'un modèle d'IA et peut ne pas être parfaite. Veuillez vérifier le résultat et apporter les corrections nécessaires.