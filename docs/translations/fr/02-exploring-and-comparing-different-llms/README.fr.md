possible to the desired output the LLM will be. This is called Prompt Engineering with Context, and it's a process of creating prompts that provide enough information to guide the model towards the desired output.

### Retrieval Augmented Generation (RAG)

RAG is a technique that combines information retrieval and LLMs to generate responses to user queries. It works by first retrieving relevant information from a database or web endpoint, and then using that information to augment the prompt given to the LLM. This way, the LLM has access to more relevant information and can generate better responses.

### Fine-tuned Model

Fine-tuning a model means training the model further on your own data. This leads to the model being more exact and responsive to your needs, but it might be costly. Fine-tuning can be done using transfer learning, where you take a pre-trained model and continue training it on your own data.

## Conclusion

In this lesson, we have explored different types of LLMs, their categorizations, and how to select the right model for your use case. We have also learned how to test and iterate with different models to understand performance on Azure, and how to improve LLM results through techniques like Prompt Engineering with Context, Retrieval Augmented Generation, and Fine-tuning. By understanding these concepts, our startup can now make informed decisions about which LLM to use for their business needs.
Pour répondre aux attentes de l'utilisateur, la réponse sera basée sur l'apprentissage "one-shot" si la demande ne comprend qu'un exemple et sur l'apprentissage "few-shot" s'il y a plusieurs exemples. L'ingénierie de la demande avec contexte est l'approche la plus rentable pour commencer. Les LLMs de génération augmentée de récupération (RAG) ont pour limitation qu'ils ne peuvent utiliser que les données qui ont été utilisées pendant leur formation pour générer une réponse. Cela signifie qu'ils ne connaissent rien des faits qui se sont produits après leur processus de formation et qu'ils ne peuvent pas accéder aux informations non publiques (comme les données d'entreprise). Cela peut être surmonté grâce à la RAG, une technique qui augmente la demande avec des données externes sous forme de morceaux de documents, en considérant les limites de longueur de la demande. Cela est pris en charge par des outils de base de données vectorielles (comme Azure Vector Search) qui récupèrent les morceaux utiles de sources de données prédéfinies et les ajoutent au contexte de la demande. Cette technique est très utile lorsqu'une entreprise n'a pas suffisamment de données, de temps ou de ressources pour affiner un LLM, mais souhaite néanmoins améliorer les performances sur une charge de travail spécifique et réduire les risques de fabrications, c'est-à-dire la mystification de la réalité ou le contenu nocif. Le fine-tuning est un processus qui utilise l'apprentissage par transfert pour "adapter" le modèle à une tâche en aval ou pour résoudre un problème spécifique. Contrairement à l'apprentissage "few-shot" et à la RAG, cela entraîne la génération d'un nouveau modèle, avec des poids et des biais mis à jour. Il nécessite un ensemble d'exemples d'entraînement composé d'une seule entrée (la demande) et de sa sortie associée (la complétion). Cela serait l'approche préférée si : - Utilisation de modèles affinés. Une entreprise souhaite utiliser des modèles d'incorporation moins performants que des modèles haute performance, ce qui donne une solution plus rentable et plus rapide. - Prise en compte de la latence. La latence est importante pour un cas d'utilisation spécifique, il n'est donc pas possible d'utiliser des demandes très longues ou le nombre d'exemples à apprendre du modèle ne correspond pas à la limite de longueur de la demande. - Rester à jour. Une entreprise dispose de nombreuses données de haute qualité et d'étiquettes de vérité de terrain et des ressources nécessaires pour maintenir ces données à jour au fil du temps. La formation d'un LLM à partir de zéro est sans aucun doute l'approche la plus difficile et la plus complexe à adopter, nécessitant des quantités massives de données, des ressources qualifiées et une puissance de calcul appropriée. Cette option ne devrait être envisagée que dans un scénario où une entreprise dispose d'un cas d'utilisation spécifique au domaine et d'une grande quantité de données centrées sur le domaine. Pour améliorer les résultats de complétion de LLM, l'ingénierie de la demande avec contexte, la RAG et le fine-tuning peuvent être de bonnes approches. La réponse est 3, si vous avez le temps, les ressources et des données de haute qualité, le fine-tuning est la meilleure option pour rester à jour. Cependant, si vous cherchez à améliorer les choses et que vous manquez de temps, il vaut la peine de considérer d'abord la RAG. Pour en savoir plus sur la façon dont vous pouvez utiliser la RAG pour votre entreprise, consultez le lien fourni. Après avoir terminé cette leçon, consultez notre collection d'apprentissage sur l'IA générative pour continuer à améliorer vos connaissances en la matière. Passez à la leçon 3 où nous verrons comment construire avec l'IA générative de manière responsable.


Avertissement : La traduction a été traduite à partir de son original par un modèle d'IA et peut ne pas être parfaite. Veuillez examiner la sortie et apporter toutes les corrections nécessaires.