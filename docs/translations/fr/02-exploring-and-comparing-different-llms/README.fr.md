possible response the LLM will provide. This is called ‚Äúprompt engineering with context‚Äù. For example, instead of asking ‚ÄúWhat is the weather like?‚Äù, a more specific prompt could be ‚ÄúWhat is the weather forecast for tomorrow in Paris?‚Äù. 

### Retrieval Augmented Generation (RAG)
Retrieval Augmented Generation (RAG) is a technique that combines information retrieval and language generation. It involves retrieving relevant information from a database or a web endpoint and then incorporating it into the generated response. This can improve the quality and relevance of the generated response, especially when the LLM is used for tasks like question answering or chatbots.

### Fine-tuning a model
Fine-tuning a pre-trained LLM with custom data is another way to improve model performance on specific workloads. This involves training the model further on your own data, which leads to the model being more accurate and responsive to your needs. However, fine-tuning a model can be costly and requires a large amount of training data.

## Conclusion
In this lesson, we explored different types of LLMs, their pros and cons, and how to test and iterate with them to understand performance on Azure. We also discussed different approaches to improving LLM results, such as prompt engineering with context, retrieval augmented generation, and fine-tuning a model. With this knowledge, our startup team can select the right LLM for their use case and deploy it in production with confidence.
Pour r√©pondre aux attentes de l'utilisateur, la r√©ponse sera adapt√©e. Dans ce cas, nous parlons d'un apprentissage "one-shot" s'il n'y a qu'un seul exemple dans la requ√™te et d'un apprentissage "few shot" s'il y a plusieurs exemples. L'ing√©nierie de la requ√™te avec le contexte est l'approche la plus rentable pour commencer. ### G√©n√©ration augment√©e de r√©cup√©ration (RAG) Les LLM de R√©cup√©ration Augment√©e de G√©n√©ration (RAG) ont pour limitation qu'ils ne peuvent utiliser que les donn√©es qui ont √©t√© utilis√©es pendant leur formation pour g√©n√©rer une r√©ponse. Cela signifie qu'ils ne savent rien des faits qui se sont produits apr√®s leur processus de formation, et qu'ils ne peuvent pas acc√©der aux informations non publiques (comme les donn√©es de l'entreprise). Cela peut √™tre surmont√© gr√¢ce √† RAG, une technique qui augmente la requ√™te avec des donn√©es externes sous forme de fragments de documents, en tenant compte des limites de longueur de la requ√™te. Cela est soutenu par des outils de base de donn√©es vectoriels (comme [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) qui r√©cup√®rent les fragments utiles √† partir de sources de donn√©es pr√©d√©finies vari√©es et les ajoutent au contexte de la requ√™te. Cette technique est tr√®s utile lorsqu'une entreprise n'a pas suffisamment de donn√©es, de temps ou de ressources pour affiner un LLM, mais souhaite n√©anmoins am√©liorer les performances sur une charge de travail sp√©cifique et r√©duire les risques de falsification, c'est-√†-dire la mystification de la r√©alit√© ou le contenu nocif. ### Mod√®le affin√© L'affinage est un processus qui exploite le transfert d'apprentissage pour "adapter" le mod√®le √† une t√¢che en aval ou pour r√©soudre un probl√®me sp√©cifique. Contrairement √† l'apprentissage few-shot et RAG, cela aboutit √† la g√©n√©ration d'un nouveau mod√®le, avec des poids et des biais mis √† jour. Il n√©cessite un ensemble d'exemples d'entra√Ænement compos√© d'une entr√©e unique (la requ√™te) et de sa sortie associ√©e (la compl√©tion). Cela serait l'approche pr√©f√©r√©e si : - **Utilisation de mod√®les affin√©s**. Une entreprise souhaite utiliser des mod√®les d'incorporation moins performants mais affin√©s plut√¥t que des mod√®les de haute performance, ce qui donne une solution plus rentable et plus rapide. - **Consid√©ration de la latence**. La latence est importante pour un cas d'utilisation sp√©cifique, il n'est donc pas possible d'utiliser des requ√™tes tr√®s longues ou le nombre d'exemples qui devraient √™tre appris √† partir du mod√®le ne correspond pas √† la limite de longueur de la requ√™te. - **Rester √† jour**. Une entreprise dispose de nombreuses donn√©es de haute qualit√© et d'√©tiquettes de v√©rit√© terrain et des ressources n√©cessaires pour maintenir ces donn√©es √† jour au fil du temps. ### Mod√®le entra√Æn√© Former un LLM √† partir de z√©ro est sans aucun doute l'approche la plus difficile et la plus complexe √† adopter, n√©cessitant des quantit√©s massives de donn√©es, des ressources qualifi√©es et une puissance de calcul appropri√©e. Cette option ne devrait √™tre envisag√©e que dans un sc√©nario o√π une entreprise dispose d'un cas d'utilisation sp√©cifique au domaine et d'une grande quantit√© de donn√©es centr√©es sur le domaine. ## V√©rification des connaissances Quelle pourrait √™tre une bonne approche pour am√©liorer les r√©sultats de compl√©tion de LLM ? 1. Ing√©nierie de la requ√™te avec le contexte 2. RAG 3. Mod√®le affin√© R: 3, si vous avez le temps et les ressources et des donn√©es de haute qualit√©, l'affinage est la meilleure option pour rester √† jour. Cependant, si vous cherchez √† am√©liorer les choses et que vous manquez de temps, il vaut la peine de consid√©rer d'abord RAG. ## üöÄ D√©fi Renseignez-vous davantage sur la fa√ßon dont vous pouvez [utiliser RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) pour votre entreprise. ## Excellent travail, continuez votre apprentissage Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d'apprentissage sur l'IA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† am√©liorer vos connaissances en mati√®re d'IA g√©n√©rative ! Rendez-vous √† la le√ßon 3 o√π nous verrons comment [construire avec l'IA g√©n√©rative de mani√®re responsable](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!


Avertissement: La traduction a √©t√© traduite √† partir de son original par un mod√®le d'IA et peut ne pas √™tre parfaite. Veuillez examiner la sortie et apporter toutes les corrections n√©cessaires.