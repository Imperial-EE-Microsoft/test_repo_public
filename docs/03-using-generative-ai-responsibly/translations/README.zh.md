# 负责任地使用生成型AI

[![负责任地使用生成型AI](./images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **视频即将推出**

AI，特别是生成型AI，很容易让人着迷，但是您需要考虑如何负责任地使用它。您需要考虑如何确保输出是公平的、无害的等等。本章旨在为您提供相关背景、考虑事项以及如何采取积极措施来改进您的AI使用。

## 介绍

本课程将涵盖：

- 为什么在构建生成型AI应用程序时应优先考虑负责任的AI。
- 负责任AI的核心原则以及它们与生成型AI的关系。
- 如何通过策略和工具将这些负责任的AI原则付诸实践。

## 学习目标

完成本课程后，您将知道：

- 在构建生成型AI应用程序时负责任AI的重要性。
- 在构建生成型AI应用程序时何时考虑和应用负责任AI的核心原则。
- 用于将负责任AI概念付诸实践的工具和策略。

## 负责任AI原则

生成型AI的兴奋感从未如此之高。这种兴奋感为这个领域带来了许多新的开发人员、关注和资金。虽然这对于任何想要使用生成型AI构建产品和公司的人来说都是非常积极的，但我们也必须负责任地前进。

在整个课程中，我们关注的是构建我们的初创公司和我们的AI教育产品。我们将使用负责任AI的原则：公平、包容性、可靠性/安全性、安全和隐私、透明度和问责制。通过这些原则，我们将探讨它们与我们在产品中使用生成型AI的关系。

## 为什么应优先考虑负责任的AI

在构建产品时，以以人为中心的方法，以用户的最佳利益为考虑重点，可以取得最好的结果。

生成型AI的独特之处在于它可以为用户创建有用的答案、信息、指导和内容。这可以在不需要许多手动步骤的情况下完成，从而可以产生非常令人印象深刻的结果。如果没有适当的规划和策略，不幸的是，它也可能导致对您的用户、产品和整个社会造成一些有害的结果。

让我们看看一些（但不是全部）可能会造成有害结果的因素：

### 幻觉

幻觉是一个术语，用来描述LLM产生的内容是完全无意义的，或者基于其他信息来源，我们知道其事实上是错误的。

例如，我们为我们的初创公司构建了一个功能，允许学生向模型提问历史问题。一个学生问了这个问题“谁是泰坦尼克号的唯一幸存者？”

模型产生了以下回答：

![提示说“谁是泰坦尼克号的唯一幸存者？”](../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(来源：[Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

这是一个非常自信和全面的答案。不幸的是，它是不正确的。即使进行了最少量的研究，人们也会发现泰坦尼克号灾难的幸存者不止一个。对于刚开始研究这个话题的学生来说，这个答案可能足够有说服力，以至于不被质疑并被视为事实。这可能导致AI系统不可靠，并对我们的初创公司的声誉产生负面影响。

对于任何给定的LLM的每次迭代，我们都看到了最小化幻觉的性能改进。即使有了这种改进，我们作为应用程序构建者和用户仍然需要注意这些限制。

### 有害内容

我们在前面的部分中介绍了LLM产生不正确或无意义响应的情况。我们需要注意的另一个风险是当模型响应有害内容时。

有害内容可以定义为：

- 提供指令或鼓励自残或伤害某些群体。
- 充满仇恨或贬低的内容。
- 指导规划任何类型的攻击或暴力行为。
- 提供有关如何查找非法内容或犯罪行为的指示。
- 显示性内容。

对于我们的初创公司，我们希望确保我们拥有正确的工具和策略，以防止学生看到此类内容。

### 公平性不足

公平性被定义为“确保AI系统没有偏见和歧视，而且它们公平平等地对待每个人。”在生成型AI的世界中，我们希望确保边缘化群体的排斥性世界观不会被模型的输出强化。

这些类型的输出不仅对于为我们的用户构建积极的产品体验是破坏性的，而且还会对社会造成进一步的伤害。作为应用程序构建者，我们在构建使用生成型AI的解决方案时应始终牢记广泛和多样化的用户群体。

## 如何负责任地使用生成型AI

现在我们已经确定了负责任的生成型AI的重要性，让我们看看我们可以采取哪些4个步骤来负责任地构建我们的AI解决方案：

![缓解循环](./images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 量化潜在的危害

在软件测试中，我们测试用户在应用程序上的预期操作。类似地，测试用户最可能使用的各种提示的多样性是衡量潜在危害的好方法。

由于我们的初创公司正在构建一种教育产品，因此准备一个教育相关提示列表会很有用。这可能涵盖特定学科、历史事实以及有关学生生活的提示。

### 缓解潜在的危害

现在是时候找到方法来防止或限制模型及其响应引起的潜在危害了。我们可以从以下4个不同层面来考虑：

![缓解层](./images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **模型**。为正确的用例选择正确的模型。更大更复杂的模型，如GPT-4，当应用于更小更具体的用例时会导致更多有害内容的风险。使用训练数据进行微调也可以降低有害内容的风险。

- **安全系统**。安全系统是在为模型提供服务的平台上设置的一组工具和配置，可以帮助缓解危害。Azure OpenAI服务上的内容过滤系统就是一个例子。系统还应检测越狱攻击和不需要的活动，如来自机器人的请求。

- **元提示**。元提示和接地是我们可以基于某些行为和信息来指导或限制模型的方法。这可以是使用系统输入来定义模型的某些限制。此外，提供更与系统范围或域相关的输出。

 还可以使用Retrieval Augmented Generation（RAG）等技术，使模型仅从一组可信源中提取信息。本课程稍后将介绍[构建搜索应用程序](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)的课程。

- **用户体验**。最后一层是用户通过我们的应用程序界面直接与模型交互的地方。通过这种方式，我们可以设计UI/UX，限制用户发送给模型的输入类型以及向用户显示的文本或图像。在部署AI应用程序时，我们还必须透明地说明我们的生成型AI应用程序可以和不能做的事情。

我们有一个完整的课程专门介绍[为AI应用程序设计UX](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **评估模型**。与LLMs合作可能具有挑战性，因为我们并不总是控制模型训练的数据。尽管如此，我们仍应始终评估模型的性能和输出。测量模型的准确性、相似性、接地性和输出的相关性仍然很重要。这有助于向利益相关者和用户提供透明度和信任。

### 操作负责任的生成型AI解决方案

构建围绕您的AI应用程序的操作实践是最后阶段。这包括与我们的初创公司的其他部分（如法律和安全）合作，以确保我们符合所有监管政策。在启动之前，我们还要制定交付、处理事件和回滚的计划，以防止对我们的用户造成任何伤害。

## 工具

虽然开发负责任的AI解决方案的工作可能看起来很多，但这是值得努力的工作。随着生成型AI领域的发展，更多的工具将帮助开发人员有效地将责任集成到其工作流程中。例如，[Azure AI内容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)可以通过API请求帮助检测有害内容和图像。

## 知识检测

要确保负责任的AI使用，您需要关注哪些事项？

1. 答案是否正确。
1. 有害使用，即AI不用于犯罪目的。
1. 确保AI没有偏见和歧视。

答案：2和3是正确的。负责任的AI帮助您考虑如何缓解有害影响和偏见等。

## 🚀 挑战

阅读有关[Azure AI内容安全](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)的信息，看看您可以采用哪些内容。

## 太棒了，继续学习

完成本课程后，请查看我们的[生成型AI学习集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，继续提高您的生成型AI知识！

请前往第4课，我们将介绍[提示工程基础知识](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)！


免责声明：该翻译是由AI模型翻译的原始内容，可能不完美。请审核输出并进行必要的更正。